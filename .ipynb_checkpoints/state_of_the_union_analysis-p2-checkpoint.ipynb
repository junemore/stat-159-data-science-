{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An analysis of the State of the Union speeches - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import shelve\n",
    "\n",
    "plt.style.use('seaborn-dark')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading some of the data created in the previous part, so we can continue where we left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses = pd.read_hdf('results/df1.h5', 'addresses')\n",
    "with shelve.open('results/vars1') as db:\n",
    "    speeches = db['speeches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check that we're getting the full set of speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, 3)\n",
      "227\n"
     ]
    }
   ],
   "source": [
    "print(addresses.shape)\n",
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text analysis\n",
    "\n",
    "Let's ask a few basic questions about this text, by populating our `addresses` dataframe with some extra information. As a reminder, so far we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1790-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1790-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1791-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1792-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1793-12-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             president                        title       date\n",
       "0    George Washington   State of the Union Address 1790-01-08\n",
       "1    George Washington   State of the Union Address 1790-12-08\n",
       "2    George Washington   State of the Union Address 1791-10-25\n",
       "3    George Washington   State of the Union Address 1792-11-06\n",
       "4    George Washington   State of the Union Address 1793-12-03"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the following information to this DF:\n",
    "\n",
    "* `n_words`: number of words in the speech\n",
    "* `n_uwords`: number of *unique* words in the speech\n",
    "* `n_swords`: number of *unique, stemmed* words in the speech\n",
    "* `n_chars`: number of letters in the speech\n",
    "* `n_sent`: number of sentences in the speech\n",
    "\n",
    "For this level of complexity, it's probably best if we go with NLTK. Remember, that `speeches` is our list with all the speeches, indexed in the same way as the `addresses` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "\n",
    "def clean_word_tokenize(doc):\n",
    "    \"\"\"custom word toenizer which removes stop words and punctuation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : string\n",
    "        A document to be tokenized\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tokens\n",
    "    \"\"\"\n",
    "    n_sent=[]\n",
    "    n_words_all=[]\n",
    "    n_words=[]\n",
    "    n_uwords=[]\n",
    "    n_swords=[]\n",
    "    n_chars=[]\n",
    "    stop=stopwords.words(\"english\") + list(string.punctuation)\n",
    "    stemmer=SnowballStemmer('english')\n",
    "    #countLetters= charCounter(string.letters)\n",
    "    for i in doc:\n",
    "        n_chars=np.append(n_chars,len(i))\n",
    "        n_sent=np.append(n_sent,len(nltk.sent_tokenize(i)))\n",
    "        n_words_all=np.append(n_words_all,len(nltk.word_tokenize(i)))\n",
    "        n_words=np.append(n_words,len([word for word in nltk.word_tokenize(i) if word not in stop]))\n",
    "        i=i.lower()\n",
    "        n_uwords=np.append(n_uwords,len(Counter([word for word in nltk.word_tokenize(i)]).keys()))\n",
    "        n_swords=np.append(n_swords,len(Counter([stemmer.stem(word) for word in [word for word in nltk.word_tokenize(i)]]).keys()))     \n",
    "    columns=['n_sent','n_words_all','n_words','n_uwords','n_swords','n_chars']\n",
    "    df=pd.DataFrame({'n_sent':n_sent,'n_words_all':n_words_all,'n_words':n_words,'n_uwords':n_uwords,'n_swords':n_swords,'n_chars':n_chars},columns=columns)          \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute these quantities for each speech, as well as saving the set of unique, stemmed words for each speech, which we'll need later to construct the complete term-document matrix across all speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>n_sent</th>\n",
       "      <th>n_words_all</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_uwords</th>\n",
       "      <th>n_swords</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1790-01-08</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>6753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1790-12-08</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>683.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>8455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1791-10-25</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2487.0</td>\n",
       "      <td>1136.0</td>\n",
       "      <td>799.0</td>\n",
       "      <td>685.0</td>\n",
       "      <td>14203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1792-11-06</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2298.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>753.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>12764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>State of the Union Address</td>\n",
       "      <td>1793-12-03</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>11696.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             president                        title       date  n_sent  \\\n",
       "0    George Washington   State of the Union Address 1790-01-08    24.0   \n",
       "1    George Washington   State of the Union Address 1790-12-08    40.0   \n",
       "2    George Washington   State of the Union Address 1791-10-25    60.0   \n",
       "3    George Washington   State of the Union Address 1792-11-06    61.0   \n",
       "4    George Washington   State of the Union Address 1793-12-03    56.0   \n",
       "\n",
       "   n_words_all  n_words  n_uwords  n_swords  n_chars  \n",
       "0       1178.0    538.0     458.0     417.0   6753.0  \n",
       "1       1515.0    683.0     583.0     528.0   8455.0  \n",
       "2       2487.0   1136.0     799.0     685.0  14203.0  \n",
       "3       2298.0   1042.0     753.0     644.0  12764.0  \n",
       "4       2132.0    972.0     783.0     718.0  11696.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=clean_word_tokenize(speeches)\n",
    "addresses=addresses.join(results)\n",
    "addresses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** difference in n_unwords, and n_swords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech_words=[]\n",
    "speeches_cleaned=[]\n",
    "stop=stopwords.words(\"english\") + list(string.punctuation)\n",
    "stemmer=SnowballStemmer('english')\n",
    "for i in np.arange(len(speeches)):\n",
    "    speeches[i]=speeches[i].lower()\n",
    "    speech_words.append(set(Counter([stemmer.stem(word) for word in nltk.word_tokenize(speeches[i])]).keys()))\n",
    "    speeches_cleaned.append([stemmer.stem(word) for word in [word for word in nltk.word_tokenize(speeches[i])]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a summary of these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.precision = 2\n",
    "addresses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing characteristics of the speeches\n",
    "\n",
    "Now we explore some of the relationships between the speeches, their authors, and time.\n",
    "\n",
    "How properties of the speeches change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Seaborn to provide a plot such as this, and discuss:\n",
    "import math \n",
    "log_n_sent=[]\n",
    "for i in addresses['n_sent']:\n",
    "    log_n_sent=np.append(log_n_sent,math.log(i,math.e))    \n",
    "log_n_words=[]\n",
    "for i in addresses['n_words']:\n",
    "    log_n_words=np.append(log_n_words,math.log(i,math.e))\n",
    "vocab_per_word=(addresses['n_swords'])/addresses['n_words']\n",
    "mean_word_length=addresses['n_chars']/addresses['n_words']\n",
    "mean_sent_length=addresses['n_words']/addresses['n_sent']\n",
    "frac_stop_word=(addresses['n_words_all']-addresses['n_words'])/addresses['n_words']\n",
    "Date=addresses['date'].dt.year\n",
    "\n",
    "f1,axarr = plt.subplots(3,2,sharex=True,linewidth=0.001)\n",
    "f1.set_figwidth(15)\n",
    "f1.set_figheight(9)\n",
    "ax=plt.gca()\n",
    "#set title and range of x axies\n",
    "plt.suptitle( 'change in speech characteristics over time')\n",
    "plt.xticks(np.arange(1808,2009,40))\n",
    "#plot[0][0], Date v.s. Log Number of Sentence\n",
    "axarr[0,0].set_ylabel(\"Value\")\n",
    "axarr[0,0].set_title(\"Log Number of Sentence\")\n",
    "axarr[0,0].plot(Date,log_n_sent)\n",
    "axarr[0,0].set_yticks(np.arange(3,8,1))\n",
    "\n",
    "#plot[0][1], Date v.s. Log Number of Words\n",
    "axarr[0,1].set_title(\"Log Number of Words\")\n",
    "axarr[0,1].plot(Date,log_n_words)\n",
    "axarr[0,1].set_yticks(np.arange(6,11,1))\n",
    "\n",
    "#plot[1][0], Date v.s. Vocabulary size per word\n",
    "axarr[1,0].set_ylabel(\"Value\")\n",
    "axarr[1,0].set_title(\"Vocabulary size per word\")\n",
    "axarr[1,0].plot(Date,vocab_per_word)\n",
    "axarr[1,0].set_yticks(np.arange(0.2,0.9,0.1)) \n",
    "\n",
    "#plot[1][1], Date v.s. Average Sentence length\n",
    "axarr[1,1].set_title(\"Average Sentence length\")\n",
    "axarr[1,1].plot(Date,mean_sent_length)\n",
    "axarr[1,1].set_yticks(np.arange(10.0,27.6,2.5)) \n",
    "\n",
    "#plot[2][0], Date v.s. Average Word length\n",
    "axarr[2,0].plot(Date,mean_word_length)\n",
    "axarr[2,0].set_title(\"Average Word length\")\n",
    "axarr[2,0].set_ylabel(\"Value\")\n",
    "axarr[2,0].set_xlabel(\"Date\")\n",
    "axarr[2,0].set_yticks(np.arange(9,13.1,0.5)) \n",
    "# ax.set_yticks(np.linspace(9,13,8))\n",
    "# ax.yaxis.set_ticks(np.arange(9, 13, 8))\n",
    "#plot[2,1], Date v.s. Fraction of stop words\n",
    "axarr[2,1].plot(Date,frac_stop_word)\n",
    "axarr[2,1].set_title(\"Fraction of stop words\")\n",
    "axarr[2,1].set_xlabel(\"Date\")\n",
    "axarr[2,1].set_yticks(np.arange(0.8,1.4,0.1))  \n",
    "f1.savefig(\"fig/speech_changes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** The graph of number of sentences and number of words seems to have a positive upward trend which means, which means the there are more and more words and sentence in the State of Union address as time approach present. ***\n",
    "\n",
    "***The graph of vocabulary size per word ,average sentence length, average word length and Fraction of stop words semms to have a negative downward trends. This could suggest that the President were choosing to use less compliate words and sentence, keeping the language plain and simple, as time goes present.***\n",
    "\n",
    "***The more detailed analysis will be included in the report***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the distributions by president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add variable to original data frame\n",
    "president=addresses['president']\n",
    "values=pd.DataFrame({\"president\":president,\"log_n_sent\":log_n_sent,\"log_n_words\":log_n_words,\"vocab_per_word\":vocab_per_word,\"mean_word_length\":mean_word_length,\"mean_sent_length\":mean_sent_length,\"frac_stop_word\":frac_stop_word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f2,axarr = plt.subplots(3,2,sharex=True)\n",
    "f2.set_figwidth(15)\n",
    "f2.set_figheight(9)\n",
    "ax=plt.gca()\n",
    "#set title and range of x axies\n",
    "plt.suptitle( 'Speech characteristics by president')\n",
    "plt.xticks(np.arange(1808,2009,40))\n",
    "\n",
    "#plot[0][0],  Log Number of Sentence by president\n",
    "sns.violinplot(data=values, x=\"president\", y=\"log_n_sent\",ax=axarr[0,0])\n",
    "axarr[0,0].set_yticks(np.arange(2,10,1))\n",
    "axarr[0,0].set_ylabel(\"Value\")\n",
    "axarr[0,0].set_xlabel(\"\")\n",
    "axarr[0,0].set_title(\"Log Number of sentences\")\n",
    "\n",
    "#plot[0][1],  Log Number of Words by president\n",
    "axarr[0,1].set_title(\"Log Number of Words\")\n",
    "sns.violinplot(data=values, x=\"president\", y=\"log_n_words\",ax=axarr[0,1])\n",
    "axarr[0,1].set_yticks(np.arange(5,13,1))\n",
    "axarr[0,1].set_ylabel(\"Value\")\n",
    "axarr[0,1].set_xlabel(\"\")\n",
    "\n",
    "#plot[1][0], Vocabulary size per word by president\n",
    "\n",
    "axarr[1,0].set_title(\"Vocabulary size per word\")\n",
    "sns.violinplot(data=values, x=\"president\", y=\"vocab_per_word\",ax=axarr[1,0])\n",
    "axarr[1,0].set_yticks(np.arange(0,1,0.2)) \n",
    "axarr[1,0].set_ylabel(\"Value\")\n",
    "axarr[1,0].set_xlabel(\"\")\n",
    "\n",
    "#plot[1][1], Average Sentence length by president\n",
    "axarr[1,1].set_title(\"Average Sentence length\")\n",
    "sns.violinplot(data=values, x=\"president\", y=\"mean_sent_length\",ax=axarr[1,1])\n",
    "axarr[1,1].set_yticks(np.arange(10,30.1,5)) \n",
    "axarr[1,1].set_ylabel(\"Value\")\n",
    "axarr[1,1].set_xlabel(\"\")\n",
    "\n",
    "#plot[2][0], Average Word length by president\n",
    "sns.violinplot(data=values, x=\"president\", y=\"mean_word_length\",ax=axarr[2,0])\n",
    "axarr[2,0].set_title(\"Average Word length\")\n",
    "axarr[2,0].set_ylabel(\"Value\")\n",
    "axarr[2,0].set_xlabel(\"Date\")\n",
    "axarr[2,0].set_yticks(np.arange(9,14,1)) \n",
    "for tick in axarr[2,0].get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "\n",
    "\n",
    "#plot[2][1], Fraction of stop words by president\n",
    "sns.violinplot(data=values, x=\"president\", y=\"frac_stop_word\",ax=axarr[2,1])\n",
    "axarr[2,1].set_title(\"Fraction of stop words\")\n",
    "axarr[2,1].set_xlabel(\"Date\")\n",
    "axarr[2,1].set_ylabel(\"Value\")\n",
    "axarr[2,1].set_yticks(np.arange(0.6,1.3,0.2))  \n",
    "for tick in axarr[2,1].get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "f2.savefig(\"fig/speech_characteristics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** The trend of the distribution by Presidents is somewhat similar to the trend in the one by Year, with the first two graph positive increassing trend and the rest of the graph slightly drcreasing trend. ***\n",
    "\n",
    "*** more detail analysis will be put in the report*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate results storage\n",
    "\n",
    "Since this may have taken a while, we now serialize the results we have for further use. Note that we don't overwrite our original dataframe file, so we can load both (even though in this notebook we reused the name `addresses`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addresses.to_hdf('results/df2.h5', 'addresses')\n",
    "\n",
    "with shelve.open('results/vars2') as db:\n",
    "    db['speech_words'] = speech_words\n",
    "    db['speeches_cleaned'] = speeches_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
